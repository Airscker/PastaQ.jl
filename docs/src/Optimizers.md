# Optimizers 

### Stochastic gradient descent
```@docs
Sgd
```

### Adagrad 
```@docs
Adagrad
```

### Adadelta 
```@docs
Adadelta
```
### Adam 
```@docs
Adam
```

## Update
```@docs
update!
```

